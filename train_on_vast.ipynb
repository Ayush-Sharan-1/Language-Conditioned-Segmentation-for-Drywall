{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Language-Conditioned Segmentation for Drywall - Vast.ai Training Setup\n",
        "\n",
        "This notebook prepares and launches training on Vast.ai.\n",
        "\n",
        "## Steps:\n",
        "1. Install dependencies (PyTorch with CUDA)\n",
        "2. Verify GPU availability\n",
        "3. Download datasets from Roboflow (COCO format)\n",
        "4. Organize dataset directories\n",
        "5. Launch the training script\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install PyTorch with CUDA support\n",
        "# Check CUDA version first, then install appropriate PyTorch\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Check CUDA version\n",
        "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "if result.returncode == 0:\n",
        "    print(\"CUDA available. Installing PyTorch with CUDA support...\")\n",
        "    # Install PyTorch with CUDA 12.1 (compatible with most modern GPUs)\n",
        "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "else:\n",
        "    print(\"Warning: nvidia-smi not found. Installing CPU-only PyTorch.\")\n",
        "    !pip install torch torchvision torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install other dependencies from requirements.txt\n",
        "!pip install pillow>=8.0.0 numpy>=1.21.0 pycocotools>=2.0.4 ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install roboflow tensorboard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Verify GPU Availability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
        "    print(\"\\n✓ GPU verification successful!\")\n",
        "else:\n",
        "    print(\"\\n✗ Warning: CUDA not available. Training will be slow on CPU.\")\n",
        "    sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download Datasets from Roboflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set up Roboflow API key (set this as environment variable on Vast.ai)\n",
        "# You can also set it here if needed: os.environ[\"ROBOFLOW_API_KEY\"] = \"your_api_key\"\n",
        "\n",
        "if \"ROBOFLOW_API_KEY\" not in os.environ:\n",
        "    print(\"Warning: ROBOFLOW_API_KEY not set. Please set it as an environment variable.\")\n",
        "    print(\"You can get your API key from: https://app.roboflow.com/\")\n",
        "    # Uncomment and set your API key here if needed:\n",
        "    # os.environ[\"ROBOFLOW_API_KEY\"] = \"your_api_key_here\"\n",
        "else:\n",
        "    print(\"✓ ROBOFLOW_API_KEY found\")\n",
        "\n",
        "from roboflow import Roboflow\n",
        "\n",
        "# Initialize Roboflow\n",
        "rf = Roboflow(api_key=os.environ[\"ROBOFLOW_API_KEY\"])\n",
        "\n",
        "# Download the cracks dataset\n",
        "# Update these values based on your Roboflow project\n",
        "workspace_name = \"Ayush-Lab\"  # Update with your workspace\n",
        "project_name = \"cracks\"  # Update with your project name\n",
        "version = 1  # Update with your dataset version\n",
        "\n",
        "print(f\"\\nDownloading dataset: {workspace_name}/{project_name} (version {version})\")\n",
        "project = rf.workspace(workspace_name).project(project_name)\n",
        "dataset = project.version(version).download(\"coco\")\n",
        "\n",
        "print(f\"\\n✓ Dataset downloaded to: {dataset.location}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Expected directory structure:\n",
        "# Dataset/\n",
        "#   cracks/\n",
        "#     train/\n",
        "#       _annotations.coco.json\n",
        "#       *.jpg\n",
        "#     valid/\n",
        "#       _annotations.coco.json\n",
        "#       *.jpg\n",
        "#   prompts.json\n",
        "\n",
        "# Find the downloaded dataset directory\n",
        "# Roboflow typically downloads to: {project_name}-{version}\n",
        "dataset_dir = Path(dataset.location)\n",
        "print(f\"Dataset location: {dataset_dir}\")\n",
        "\n",
        "# Check if train/valid directories exist\n",
        "if (dataset_dir / \"train\").exists() and (dataset_dir / \"valid\").exists():\n",
        "    print(\"✓ Train/valid splits found\")\n",
        "    \n",
        "    # Create Dataset directory structure\n",
        "    target_dir = Path(\"Dataset/cracks\")\n",
        "    target_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Copy train split\n",
        "    train_source = dataset_dir / \"train\"\n",
        "    train_target = target_dir / \"train\"\n",
        "    if train_target.exists():\n",
        "        shutil.rmtree(train_target)\n",
        "    shutil.copytree(train_source, train_target)\n",
        "    print(f\"✓ Copied train split to {train_target}\")\n",
        "    \n",
        "    # Copy valid split\n",
        "    valid_source = dataset_dir / \"valid\"\n",
        "    valid_target = target_dir / \"valid\"\n",
        "    if valid_target.exists():\n",
        "        shutil.rmtree(valid_target)\n",
        "    shutil.copytree(valid_source, valid_target)\n",
        "    print(f\"✓ Copied valid split to {valid_target}\")\n",
        "    \n",
        "    # Verify COCO annotation files exist\n",
        "    train_annotations = train_target / \"_annotations.coco.json\"\n",
        "    valid_annotations = valid_target / \"_annotations.coco.json\"\n",
        "    \n",
        "    if train_annotations.exists():\n",
        "        print(f\"✓ Train annotations found: {train_annotations}\")\n",
        "    else:\n",
        "        print(f\"✗ Warning: Train annotations not found at {train_annotations}\")\n",
        "    \n",
        "    if valid_annotations.exists():\n",
        "        print(f\"✓ Valid annotations found: {valid_annotations}\")\n",
        "    else:\n",
        "        print(f\"✗ Warning: Valid annotations not found at {valid_annotations}\")\n",
        "    \n",
        "    # Count images\n",
        "    train_images = list(train_target.glob(\"*.jpg\")) + list(train_target.glob(\"*.png\"))\n",
        "    valid_images = list(valid_target.glob(\"*.jpg\")) + list(valid_target.glob(\"*.png\"))\n",
        "    print(f\"\\nTrain images: {len(train_images)}\")\n",
        "    print(f\"Valid images: {len(valid_images)}\")\n",
        "    \n",
        "else:\n",
        "    print(\"✗ Error: Train/valid splits not found in downloaded dataset\")\n",
        "    print(f\"Available directories: {list(dataset_dir.iterdir())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create prompts.json file if it doesn't exist\n",
        "import json\n",
        "\n",
        "prompts_path = Path(\"Dataset/prompts.json\")\n",
        "prompts_dir = prompts_path.parent\n",
        "prompts_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not prompts_path.exists():\n",
        "    # Default prompts for crack segmentation\n",
        "    default_prompts = {\n",
        "        \"crack\": [\n",
        "            \"segment crack\",\n",
        "            \"segment cracks\",\n",
        "            \"segment wall crack\",\n",
        "            \"segment wall cracks\",\n",
        "            \"segment surface crack\",\n",
        "            \"segment surface cracks\",\n",
        "            \"segment drywall crack\",\n",
        "            \"segment drywall cracks\",\n",
        "            \"find crack\",\n",
        "            \"find cracks\",\n",
        "            \"detect crack region\",\n",
        "            \"detect crack regions\"\n",
        "        ],\n",
        "        \"taping_area\": [\n",
        "            \"segment taping area\",\n",
        "            \"segment drywall tape\",\n",
        "            \"segment wall joint\",\n",
        "            \"segment drywall seam\",\n",
        "            \"segment taped joint\",\n",
        "            \"segment joint area\",\n",
        "            \"find drywall seam\",\n",
        "            \"detect taping region\"\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    with open(prompts_path, 'w') as f:\n",
        "        json.dump(default_prompts, f, indent=2)\n",
        "    print(f\"✓ Created prompts.json at {prompts_path}\")\n",
        "else:\n",
        "    print(f\"✓ prompts.json already exists at {prompts_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Verify Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify all required files and directories exist\n",
        "required_paths = [\n",
        "    Path(\"Dataset/cracks/train/_annotations.coco.json\"),\n",
        "    Path(\"Dataset/cracks/valid/_annotations.coco.json\"),\n",
        "    Path(\"Dataset/prompts.json\"),\n",
        "    Path(\"scripts/train_cracks.py\"),\n",
        "    Path(\"scripts/model.py\"),\n",
        "    Path(\"scripts/dataset.py\"),\n",
        "    Path(\"scripts/losses.py\"),\n",
        "    Path(\"scripts/utils.py\")\n",
        "]\n",
        "\n",
        "print(\"Verifying setup...\")\n",
        "all_good = True\n",
        "for path in required_paths:\n",
        "    if path.exists():\n",
        "        print(f\"✓ {path}\")\n",
        "    else:\n",
        "        print(f\"✗ Missing: {path}\")\n",
        "        all_good = False\n",
        "\n",
        "if all_good:\n",
        "    print(\"\\n✓ All required files found!\")\n",
        "else:\n",
        "    print(\"\\n✗ Some files are missing. Please check your setup.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Launch Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "training_config = {\n",
        "    \"data_root\": \"Dataset/cracks\",\n",
        "    \"prompts_path\": \"Dataset/prompts.json\",\n",
        "    \"batch_size\": 8,  # Adjust based on GPU memory\n",
        "    \"num_epochs\": 50,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"seed\": 42,\n",
        "    \"clip_model\": \"ViT-B/32\",\n",
        "    \"freeze_encoder\": True,  # Recommended for initial training\n",
        "    \"output_dir\": \"checkpoints\",\n",
        "    \"save_predictions\": False,  # Set to True to save predictions after training\n",
        "    \"val_every_n_epochs\": 1  # Validate every N epochs\n",
        "}\n",
        "\n",
        "print(\"Training configuration:\")\n",
        "for key, value in training_config.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build training command\n",
        "cmd_parts = [\"python\", \"scripts/train_cracks.py\"]\n",
        "\n",
        "for key, value in training_config.items():\n",
        "    if isinstance(value, bool):\n",
        "        if value:\n",
        "            cmd_parts.append(f\"--{key}\")\n",
        "    else:\n",
        "        cmd_parts.append(f\"--{key}\")\n",
        "        cmd_parts.append(str(value))\n",
        "\n",
        "cmd = \" \".join(cmd_parts)\n",
        "print(f\"\\nLaunching training with command:\\n{cmd}\\n\")\n",
        "\n",
        "# Launch training\n",
        "import subprocess\n",
        "process = subprocess.Popen(\n",
        "    cmd_parts,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    universal_newlines=True,\n",
        "    bufsize=1\n",
        ")\n",
        "\n",
        "# Stream output\n",
        "for line in process.stdout:\n",
        "    print(line, end='')\n",
        "\n",
        "process.wait()\n",
        "\n",
        "if process.returncode == 0:\n",
        "    print(\"\\n✓ Training completed successfully!\")\n",
        "else:\n",
        "    print(f\"\\n✗ Training failed with exit code {process.returncode}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative: Run Training in Background\n",
        "\n",
        "If you want to run training in the background and continue using the notebook:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to run training in background\n",
        "# import subprocess\n",
        "# import os\n",
        "# \n",
        "# log_file = \"training.log\"\n",
        "# with open(log_file, 'w') as f:\n",
        "#     process = subprocess.Popen(\n",
        "#         cmd_parts,\n",
        "#         stdout=f,\n",
        "#         stderr=subprocess.STDOUT\n",
        "#     )\n",
        "# \n",
        "# print(f\"Training started in background. Process ID: {process.pid}\")\n",
        "# print(f\"Logs are being written to: {log_file}\")\n",
        "# print(f\"Monitor progress with: tail -f {log_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "- Make sure to set `ROBOFLOW_API_KEY` as an environment variable on Vast.ai\n",
        "- Adjust `batch_size` based on your GPU memory (start with 4-8)\n",
        "- Training logs will be saved to `runs/` directory (view with TensorBoard)\n",
        "- Model checkpoints will be saved to `checkpoints/` directory\n",
        "- Best model will be saved as `checkpoints/best_model.pth`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
